<!-- AUTOGENERATED: do not edit by hand. -->
# Copilot Instructions — framecodex

## Project charter
This repository is a deterministic publication + documentation pipeline over GF0 frames.
Generated documentation and publication artifacts MUST be reproducible.

## Non-negotiable invariants
- Frames are the single source of truth (`frames/**/v*/frame.yml`).
- Generated docs (`docs/**/README.md`, `docs/MANIFEST.json`) are produced by the pipeline, not edited manually.
- `out/` is transient and MUST remain gitignored.

## Repo laws (normative)
- **InlineMarkup-K1**: `law://repo/text/inline-markup-k1` v0.1.0 (`frames/repo/law/text/inline-markup-k1/v0.1.0/frame.yml`)
- **RepoLaw-K1**: `law://repo/governance/repo-law-k1` v0.1.0 (`frames/repo/law/governance/repo-law-k1/v0.1.0/frame.yml`)

## Golden gates (run before/after changes)
- `tools/enforce_repo_law/run` (policy + validators)
- `tools/render_docs/run` (regenerate docs deterministically)
- `tools/no_diff/run` (reproducibility check)

## Tool entrypoints
- `build_alias_index`: `tools/build_alias_index/run`
- `enforce_repo_law`: `tools/enforce_repo_law/run`
- `gen_copilot_instructions`: `tools/gen_copilot_instructions/run`
- `gen_index`: `tools/gen_index/run`
- `markup_audit`: `tools/markup_audit/run`
- `no_diff`: `tools/no_diff/run`
- `pub_build_pdf`: `tools/pub_build_pdf/run`
- `pub_manifest`: `tools/pub_manifest/run`
- `render_docir`: `tools/render_docir/run`
- `render_docs`: `tools/render_docs/run`
- `render_latex_spec`: `tools/render_latex_spec/run`
- `render_md_doc`: `tools/render_md_doc/run`
- `render_simple_md`: `tools/render_simple_md/run`
- `render_tex_doc`: `tools/render_tex_doc/run`
- `run_with_timeout`: `tools/run_with_timeout/run`
- `semantic_invariants`: `tools/semantic_invariants/run`
- `validate_group`: `tools/validate_group/run`
- `validate_inline_markup`: `tools/validate_inline_markup/run`
- `validate_references`: `tools/validate_references/run`

## CI workflows
- `.github/workflows/docir-smoke.yml` — docir-smoke
  - job `smoke`
    - step: Render DocIR, Markdown, and LaTeX (smoke)
    - step: Upload rendered outputs (artifact)
- `.github/workflows/index-check.yml` — index-check
  - job `verify`
    - step: Generate INDEX.md
    - step: Verify INDEX.md is up to date
- `.github/workflows/pub-docs.yml` — pub-docs
  - job `build`
    - step: Install tectonic (PDF build tool)
    - step: Run core repo gates
    - step: Build publication artifacts (DocIR → LaTeX + PDF + MANIFEST)
    - step: Render human Markdown specs (DocIR → Markdown)
    - step: Prepare GitHub Pages content
    - step: Deploy to gh-pages
    - step: Upload pub artifacts
- `.github/workflows/release-archive.yml` — release-archive
  - job `validate`
    - step: Run repo gates (time-bounded)
    - step: Verify citation and Zenodo metadata files exist
    - step: Upload release manifest artifacts
- `.github/workflows/tag-release.yml` — tag-release
  - job `build-and-release`
    - step: Install tectonic (PDF build tool)
    - step: Run core repo gates
    - step: Build publication artifacts (DocIR → LaTeX + PDF + MANIFEST)
    - step: Package release assets
    - step: Create GitHub Release and upload assets

## How to work in this repo
- Prefer editing source frames and tools; then regenerate docs.
- If asked to change a generated README, locate its frame under `frames/**/v*/frame.yml` and edit there.
- Keep builds deterministic: avoid timestamps, randomness, and network access in generators.

## Context pack (autogenerated excerpts)
This section embeds authoritative excerpts from repo laws and tooling to provide local, offline context for agents.

### RepoLaw K1 (condensed excerpt)
Source: `frames/repo/law/governance/repo-law-k1/v0.1.0/frame.yml`

#### Charter

#### Canonical Repository Paths

#### Frames Tree and Fractal Sharding

#### DocGroup Selection

#### CI Gates

#### Receipts and Manifests

#### Required Violation Codes

### InlineMarkup-K1 (primer excerpt)
Source: `frames/repo/law/text/inline-markup-k1/v0.1.0/frame.yml`

#### Charter

InlineMarkup-K1 is a **deterministic** and **constrained** markup subset for GF0 freeform
text fields.

It is designed to:

```
- eliminate ambiguous CommonMark behaviors
- guarantee deterministic rendering across Markdown and LaTeX
- prevent unsafe raw-HTML injection in documentation outputs
```

InlineMarkup-K1 is intentionally small; authors MUST prefer frame structure (sections,
paragraphs, lists as structure where available) over rich Markdown syntax.

#### Text Format Modes

A freeform text field that participates in InlineMarkup-K1 MUST declare a `text.format`
mode:

```
- plain: treat as raw text (no markup parsing)
- md-inline: parse as a single paragraph with inline constructs
- md-block: parse paragraphs and fenced code blocks; inline constructs allowed inside paragraphs
```

#### Allowed Constructs (v0.1)

Inline constructs (v0.1):

```
- strong: **x**
- emphasis: *x* or _x_
- inline code: `x`
- inline math: $x$
- links: [label](url)
```

Block constructs (md-block only):

```
- fenced code blocks: ```lang\n...\n```
- paragraphs separated by blank lines
```

#### Disallowed Constructs

The following are disallowed under this law:

```
- raw HTML tags (validator MUST reject HTML-tag-like sequences)
- headings, tables, and other rich Markdown block constructs
```

Note: Angle brackets used for grammar/metasyntax (e.g., `<scheme>`) are not intrinsically
disallowed; only HTML-tag-like sequences are rejected.

#### Validation Requirements

The repository MUST validate InlineMarkup-K1 wherever it is enabled.

Minimum required violation codes (stable identifiers):

```
- TEXT.E.HTML_DISALLOWED
- TEXT.E.BAD_TEXT_FORMAT
- TEXT.E.BAD_CODEFENCE
```

Validation MUST be deterministic and MUST NOT depend on locale, time, network, or unstable
parser behaviors.

#### Deterministic Rendering Requirements

Render projections MUST be derived from a structured MarkupIR AST (not raw Markdown).

```
- DocIR projection MUST include the MarkupIR structure for InlineMarkup-K1 fields
- Markdown rendering MUST be deterministic from MarkupIR
- LaTeX rendering MUST be deterministic from MarkupIR
```

#### Repo Integration

Repo enforcement MUST run InlineMarkup-K1 validation as part of repo law gates.

This law is intended to be applied under RepoLaw K1 (see: `law://repo/governance/repo-law-k1`).

Specifically:

```
- tools/enforce_repo_law/run MUST execute tools/validate_inline_markup/run.py
- CI MUST fail if InlineMarkup-K1 validation fails
```

#### Changelog

- v0.1.0 (2026-01-04): initial normative definition and enforcement requirements.

#### Examples

This clause demonstrates InlineMarkup-K1 rendering across targets.

Inline:
- strong: **bold**
- emphasis: *emph*
- code: `x := 1`
- math: $E=mc^2$
- link: [FrameURL K1](law://_kernel/id/frameurl-k1)

Block code fence:

```python
def f(x: int) -> int:
    return x + 1
```

### GF0-K1 (excerpt)
Source: `frames/_kernel/spec/gf/gf0-k1/v0.3.0/frame.yml`

#### Overview

#### GraphFrame Structure

GraphFrameK0.attrs is an ordered slice of AttrK0 representing frame-level metadata (e.g. domain tags, doc build hints, repository routing hints, provenance). It MUST NOT be used to encode structural graph semantics; structural semantics are encoded only by nodes and edges. Duplicate keys are allowed and order is significant (list semantics).

A GraphFrameK0 value MUST have the following top-level fields:
  - graph_id: non-empty string identifying the frame;
  - version: non-empty string identifying the frame's version (logical or semantic);
  - attrs: list of AttrK0 values (possibly empty);
  - nodes: list of NodeK0 values (possibly empty);
  - edges: list of EdgeK0 values (possibly empty);
  - meta: list of MetaGraph values (GraphFrameK0 instances), possibly empty.
These fields MUST be present in the canonical form. Empty lists MUST be encoded as [] and MUST NOT be encoded as null or omitted.

graph_id is a logical identifier for the frame. It MUST be stable within a given repository or namespace. Different versions of the same conceptual graph SHOULD share the same graph_id but use distinct version values.

GraphFrameK0: Minimal graph container with fields {graph_id, version, attrs, nodes, edges, meta}, where nodes and edges follow NodeK0 and EdgeK0, and meta is a list of sub-GraphFrameK0 instances.

#### NodeK0 Structure

AttrK0 and MetricK0 collections MUST be represented as ordered slices, not maps. This ensures deterministic ordering and stable serialization across implementations.

AttrK0 MUST at least contain:
  - key: non-empty string;
  - value: string (UTF-8).
It MAY contain:
  - vtype: optional string naming the logical type (e.g. "string", "int", "target_ref");
  - desc: optional description string.

MetricK0 MUST at least contain:
  - name: non-empty string;
  - value: numeric value (e.g. float64).
It MAY contain:
  - unit: optional string;
  - desc: optional description.

Node attrs and metrics MUST be stored as slices and MUST NOT be represented as maps in the canonical form. Keys and names MUST be non-empty. The interpretation of specific keys is delegated to higher-level specs (SpecFrame, TaskFrame, etc.).

A NodeK0 MUST have:
  - id: non-empty string, unique within the containing GraphFrameK0;
  - kind: non-empty string describing the node's semantic role (e.g. spec, section, kernel);
  - label: optional human-readable string;
  - attrs: optional list of AttrK0;
  - metrics: optional list of MetricK0.
The set of allowed NodeK0.kind values is not constrained by GF0; higher-level specs (SpecFrame, TaskFrame, etc.) MUST define their own allowed kinds.

AttrK0: Simple key–value attribute struct with optional type and description, stored in a deterministic slice.

MetricK0: Simple name–value metric struct with optional unit and description, stored in a deterministic slice.

NodeK0: Node in a GraphFrameK0 with an ID, kind, optional label, and optional attrs/metrics slices. Node IDs are unique within a given GraphFrameK0.

#### EdgeK0 Structure

An EdgeK0 MUST have:
  - from: NodeK0 ID (string) in the same GraphFrameK0;
  - to: NodeK0 ID (string) in the same GraphFrameK0;
  - type: non-empty string describing the edge semantics (e.g. contains, depends_on);
It MAY have:
  - id: optional string identifier;
  - attrs: optional list of AttrK0;
  - metrics: optional list of MetricK0.
GF0 does not constrain the set of EdgeK0.type values beyond non-empty strings; higher- level specs MUST define allowed edge types where needed.

EdgeK0.from and EdgeK0.to MUST reference existing NodeK0 IDs in the same GraphFrameK0. Edges that reference missing nodes violate structural integrity.

EdgeK0: Directed edge in a GraphFrameK0 with from, to, and type fields, and optional ID and attrs/metrics. from/to refer to NodeK0 IDs in the same frame.

#### Fractal Meta Graphs

The meta field of a GraphFrameK0 is a list of subgraphs, each of which is itself a GraphFrameK0 with the same fields {graph_id, version, attrs, nodes, edges, meta}. This recursive structure allows auxiliary views, indexes, or annotations to be attached without changing the primary frame.

MetaGraphs MUST be structurally independent: their node IDs and edges are scoped within the subgraph. References from a MetaGraph into the parent graph MUST be expressed via attributes (e.g. parent_node_id) or well-defined edge types with explicit semantics.

Common uses of meta include: alternative layout graphs, index structures, commentary layers, or regime annotations. Higher-level specs SHOULD document their usage of meta explicitly rather than overloading it.

MetaGraph: Subgraph stored in the meta list of a GraphFrameK0. Each MetaGraph is itself a GraphFrameK0 and can carry auxiliary structure or views without changing the primary frame.

#### Invariants and Validation

A GraphFrameK0 validator MUST enforce clause.edgek0.integrity: for every EdgeK0, from and to MUST reference existing NodeK0 IDs in the same frame. Missing endpoints MUST cause validation failure.

graph_id and version MUST be non-empty strings. Frames with empty graph_id or version MUST be rejected.

Validation of a GraphFrameK0 MUST recursively validate each MetaGraph according to the same GF0 rules. Implementations MUST protect against unbounded recursion (e.g. cycles via references) and MAY impose a maximum meta depth.

Within a single GraphFrameK0, all NodeK0 IDs MUST be unique. Duplicate node IDs MUST cause validation failure.

#### Extension and Specialization

Implementations MAY embed or derive GF0 graphs from canon.Graph values. However, GF0 is defined at the YAML/JSON frame layer and does not require a one-to-one mapping to canon.Graph. When such a mapping exists, its semantics SHOULD be specified in a separate SpecFrame.

Higher-level schemas (SpecFrame, TaskFrame, EvidenceFrame, KernelCore, etc.) MUST specialize GF0 by constraining NodeK0.kind, EdgeK0.type, and attribute conventions, rather than redefining graph structure. GF0 remains the single canonical graph schema.

### SpecFrame-K1 (excerpt)
Source: `frames/_kernel/spec/spec/specframe-k1/v0.3.0/frame.yml`

#### Scope and Intent

SpecFrame K1 specifies the canonical schema for representing specifications as GraphFrame K0 graphs. A SpecFrame is any GF0 graph whose root node has kind = 'spec' and profile = 'specframe-k1'. All such graphs MUST conform to the node, edge, and attribute conventions defined in this spec.

SpecFrame K1 is intended for: (a) human spec authors, (b) GraphBrain kernels that load, validate, and refactor specs, and (c) LLM workers that generate or update specs from code, docs, or other frames.

#### Node Kinds

Within a SpecFrame, NodeK0.kind MUST be one of:
  - 'spec'      : the root specification node (exactly one per SpecFrame),
  - 'section'   : top-level or nested sections grouping terms and clauses,
  - 'term'      : definitions of key concepts,
  - 'clause'    : normative or informative statements,
  - 'property'  : small structured facts (enums, lists, thresholds),
  - 'example'   : worked examples illustrating other nodes,
  - 'spec_ref'  : references to other specs or frames.
Any other value MUST be reported as a validation error.

Each SpecFrame MUST contain exactly one node with:
  - id == graph_id,
  - kind == 'spec',
  - status in {'normative', 'informative', 'experimental'}.
This node is the root of the spec and is the unique entry point for reachability and top-level attributes.

#### Edge Types

'contains' edges encode the structural tree of the spec. The root 'spec' node MUST contain one or more 'section' nodes. Section nodes MAY contain other sections, terms, clauses, properties, and examples. Contains edges MUST form an acyclic tree (or forest) rooted at the spec node.

Within a SpecFrame, EdgeK0.type MUST be one of:
  - 'contains'   : structural containment / hierarchy,
  - 'depends_on' : spec-level dependency on another node or spec,
  - 'defines'    : term or clause defines another concept,
  - 'refines'    : clause refines or tightens another clause,
  - 'refers_to'  : non-normative reference to another node or spec,
  - 'example_of' : examples illustrating a term or clause.
Any other value MUST be reported as a validation error.

#### Attributes

A node with kind == 'clause' MUST provide:
  - 'label' : short handle for the clause,
  - 'status': SpecStatus.
It SHOULD provide:
  - 'text'  : full clause text in natural language.

A node with kind == 'example' MUST provide:
  - 'label' : short identifier for the example,
  - 'status': SpecStatus (typically 'informative').
It SHOULD provide:
  - 'text'  : free-form example text or code snippet.

A node with kind == 'property' MUST provide:
  - 'label' : short name of the property,
  - 'status': SpecStatus.
Property nodes MAY carry arbitrary additional attributes (lists, enums, thresholds) that are interpreted by tooling.

A SpecFrame validator MUST treat missing required attributes as a hard validation error. Required attributes per kind are:
  - spec     : title, status, summary, profile
  - section  : title, status
  - term     : label, status
  - clause   : label, status
  - property : label, status
  - example  : label, status
  - spec_ref : target_graph_id

A node with kind == 'section' MUST provide:
  - 'title' : short section title,
  - 'status': SpecStatus.
It SHOULD provide:
  - 'order' : integer for ordering sections within the spec.
Sections MAY nest other sections via 'contains'.

A node with kind == 'spec' MUST provide at least:
  - 'title'   : short human-readable title,
  - 'status'  : SpecStatus,
  - 'summary' : short description of the spec's scope,
  - 'profile' : string identifying the spec profile, e.g. 'specframe-k1'.
The spec node MAY also include 'version_note', 'domain', and additional profile-specific attributes.

A node with kind == 'spec_ref' MUST provide:
  - 'target_graph_id' : canonical graph_id of the referenced spec or frame.
It MAY provide:
  - 'label' : short human-readable label,
  - 'note'  : explanatory text about the reference.

For all nodes in a SpecFrame, the status attribute MUST be one of:
  - 'normative',
  - 'informative',
  - 'experimental'.

A node with kind == 'term' MUST provide:
  - 'label' : short name of the term,
  - 'status': SpecStatus.
The primary definition text MAY be stored in 'text'. Terms are usually linked via 'defines' edges from clauses that define them.

#### Validation Invariants

'contains' edges MUST form an acyclic tree (or forest) rooted at the spec node. Cycles or multiple parents for the same node via 'contains' are considered hard validation failures.

A SpecFrame validator MUST reject any edge whose type attribute is not in the allowed set specified by property.edge_types. Unknown edge types are considered hard validation failures.

A SpecFrame validator MUST reject any node whose kind attribute is not in the allowed set specified by property.node_kinds. Unknown or misspelled kinds are considered hard validation failures.

All normative nodes in a SpecFrame SHOULD be reachable from the root spec node via one or more 'contains' edges. Unreachable normative nodes SHOULD be treated as errors or at least strong warnings by tooling.

#### Integration and Usage

In SpecFrames, frame-level metadata (publish routing, domain tags, dependency tags, audience tags) SHOULD be stored in GraphFrameK0.attrs. GraphFrameK0.meta MUST be used only for true MetaGraphs (aux layout/index/view graphs) as defined by GF0. A SpecFrame validator MUST NOT require any particular GraphFrameK0.attrs keys; attrs is tooling- facing metadata and does not affect the node/edge validation rules.

Tooling MAY adopt the following conventional GraphFrameK0.attrs keys for SpecFrames:
  - domain
  - depends_on (repeatable)
  - intended_consumer (repeatable)
  - publish.root
  - publish.path
  - publish.slug

GraphBrain and specgen SHOULD treat SpecFrame K1 as the canonical schema for specs. Specs for other domains (canon, CBF, GSKernel, TaskFrame, EvidenceFrame, KernelCore, regimes) SHOULD be represented as SpecFrames and validated against this schema so that they can be composed, diffed, and refactored uniformly.

### Tool excerpts (headers)
The following are short excerpts from tool entrypoints for quick orientation.

#### tools/build_alias_index
Source: `tools/build_alias_index/run.py`

```
#!/usr/bin/env python3
"""(DEPRECATED) build_alias_index

Alias infrastructure has been removed from this repository.

FrameURL is the only normative identifier.

This legacy tool intentionally fails if invoked.
"""

from __future__ import annotations

import sys


def main() -> int:
    print(
        "ERROR: alias infrastructure has been removed; FrameURL is the only identifier.\n"
        "ERROR: tools/build_alias_index is deprecated and should not be invoked.",
        file=sys.stderr,
    )
    return 2


if __name__ == '__main__':
    raise SystemExit(main())
```

#### tools/enforce_repo_law
Source: `tools/enforce_repo_law/run`

```
#!/usr/bin/env bash
set -euo pipefail

repo_root="$(cd "$(dirname "$0")/../.." && pwd)"
out_dir="$repo_root/out/enforce_repo_law"
mkdir -p "$out_dir"

report="$out_dir/report.json"

# Minimal deterministic checks per RepoLaw K1 (structure).

missing=()
required=(
  "frames"
  "governance/ACTIVE.yml"
  "ci/contract.yml"
  "tools"
  "out"
  ".gitignore"
  "README.md"
  "LICENSE"
)

for p in "${required[@]}"; do
  if [ ! -e "$repo_root/$p" ]; then
    missing+=("$p")
  fi
done

# Check out/ is gitignored (best-effort): look for an exact line 'out/'
out_ignored="false"
if [ -f "$repo_root/.gitignore" ]; then
  if grep -Fxq "out/" "$repo_root/.gitignore"; then
    out_ignored="true"
  fi
fi

# InlineMarkup-K1 validation (deterministic, policy-driven).
./tools/validate_inline_markup/run

# Copilot instruction file (strict autogen; must match generator output)
"$repo_root/tools/gen_copilot_instructions/run"
if git -C "$repo_root" diff --exit-code -- .github/copilot-instructions.md >/dev/null; then
  :
else
  echo "ERROR: .github/copilot-instructions.md is out of date; run tools/gen_copilot_instructions/run and commit the result." >&2
  exit 1
fi

# Emit deterministic JSON report
{
  printf '{\n'
  printf '  "tool": {"id": "enforce_repo_law", "version": "0.1.2"},\n'
  printf '  "law": {"graph_id": "law://repo/governance/repo-law-k1", "version": "0.1.0"},\n'
  printf '  "ok": %s,\n' "$([ ${#missing[@]} -eq 0 ] && [ "$out_ignored" = "true" ] && echo true || echo false)"
  printf '  "checks": {\n'
  printf '    "required_paths_missing": ['
  for i in "${!missing[@]}"; do
    [ $i -gt 0 ] && printf ','
    printf '%s' "\"${missing[$i]}\""
  done
  printf '],\n'
  printf '    "out_gitignored": %s\n' "$out_ignored"
  printf '  }\n'
  printf '}\n'
} > "$report"

# Shell exit code: fail if not ok
if [ ${#missing[@]} -ne 0 ] || [ "$out_ignored" != "true" ]; then
  exit 1
fi
```

#### tools/gen_copilot_instructions
Source: `tools/gen_copilot_instructions/run.py`

```
#!/usr/bin/env python3
"""Generate .github/copilot-instructions.md deterministically.

Strict autogen:
- The output file is entirely generated; do not edit by hand.

What gets embedded:
- High-level repo invariants + golden gates
- Normative law pointers (RepoLaw K1, InlineMarkup-K1)
- Tool entrypoints
- CI workflows (jobs + step names)
- Context pack (large): excerpts from key law frames + tool headers

Determinism:
- stable ordering
- offline only
- no timestamps
- deterministic truncation by max_chars budget

This file is intended to be large (up to a budget) so coding agents have
sufficient context without external browsing.
"""

from __future__ import annotations

import argparse
import hashlib
import json
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import yaml


REPO_ROOT = Path(__file__).resolve().parents[2]


def sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def read_yaml(path: Path) -> Any:
    return yaml.safe_load(path.read_text(encoding="utf-8"))


def find_frame(path: Path) -> Tuple[str, str]:
    """Return (graph_id, version) for a GF0 frame.yml."""
    g = read_yaml(path)
    if not isinstance(g, dict):
        raise ValueError(f"frame is not a mapping: {path}")
    gid = g.get("graph_id")
    ver = g.get("version")
    if not isinstance(gid, str) or not isinstance(ver, str) or not gid or not ver:
        raise ValueError(f"missing graph_id/version: {path}")
    return gid, ver


def list_tools() -> List[Dict[str, str]]:
    out: List[Dict[str, str]] = []
    tools_dir = REPO_ROOT / "tools"
    if not tools_dir.exists():
        return out

    for d in sorted([p for p in tools_dir.iterdir() if p.is_dir()], key=lambda p: p.name):
        run = d / "run"
        run_py = d / "run.py"
        if run.exists() and run.is_file():
            out.append({"id": d.name, "entrypoint": str(run.relative_to(REPO_ROOT))})
        elif run_py.exists() and run_py.is_file():
            out.append({"id": d.name, "entrypoint": str(run_py.relative_to(REPO_ROOT))})

    return out


def _read_file_head(path: Path, *, max_lines: int) -> str:
    try:
        lines = path.read_text(encoding="utf-8").splitlines()
    except Exception:
        return ""
    head = "\n".join(lines[:max_lines]).rstrip() + "\n"
```

#### tools/gen_index
Source: `tools/gen_index/run.py`

```
#!/usr/bin/env python3
"""Generate a human-friendly root index of frames and publications.

Writes `INDEX.md` at repo root.

Design goals:
- Deterministic output (stable ordering).
- No external dependencies.
- Best-effort: includes a "Core publications" section based on `pub.*` attrs
  found on root `spec` nodes.

This is intentionally conservative: it only relies on:
- `governance/publications/registry.yml`
- `frames/**/frame.yml`

If you later want richer navigation (grouping by domain/version), extend the
`collect_frames()` step.
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple


REPO_ROOT = Path(__file__).resolve().parents[2]


def read_text(path: Path) -> str:
    return path.read_text(encoding="utf-8")


def load_yaml_minimal(path: Path) -> Any:
    """Minimal YAML loader.

    This repo already depends on YAML in other tooling, but this generator should
    not assume PyYAML is installed. We therefore only parse the small registry
    file using a very small subset.

    Supported:
    - top-level mappings
    - nested mappings
    - scalar strings

    If parsing fails, we raise with a clear message.

    NOTE: If you prefer, we can switch this to PyYAML and add it as a dependency.
    """

    # Very small indentation-based parser for `registry.yml` shape.
    lines = [ln.rstrip("\n") for ln in read_text(path).splitlines()]
    stack: List[Tuple[int, Dict[str, Any]]] = [(0, {})]

    for raw in lines:
        line = raw.rstrip()
        if not line or line.lstrip().startswith("#"):
            continue
        if ":" not in line:
            raise ValueError(f"Unsupported YAML line (no ':'): {raw}")

        indent = len(raw) - len(raw.lstrip(" "))
        key, rest = line.split(":", 1)
        key = key.strip()
        value = rest.strip()

        # find parent
        while stack and indent < stack[-1][0]:
            stack.pop()
        if not stack:
            raise ValueError(f"Bad indentation in {path}: {raw}")

        cur = stack[-1][1]

        if value == "":
            nxt: Dict[str, Any] = {}
            cur[key] = nxt
            stack.append((indent + 2, nxt))
        else:
```

#### tools/markup_audit
Source: `tools/markup_audit/run.py`

```
#!/usr/bin/env python3
"""Inventory and audit freeform text fields for InlineMarkup-K1 adoption.

This tool scans all frames and reports:
- Which nodes have text/summary/title/label/desc fields
- Current text.format usage
- Obvious candidates for markup (code patterns, paths, emphasis, etc.)

Usage:
  tools/markup_audit/run.py [--out <report.json>]

Writes:
  out/markup_audit/report.json - structured report
  out/markup_audit/candidates.csv - human-readable candidate list

Design:
- Deterministic (stable ordering)
- No modifications, scan-only
- Best-effort heuristics for code patterns, paths, refs
"""

from __future__ import annotations

import csv
import json
import re
import sys
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import yaml

REPO_ROOT = Path(__file__).resolve().parents[2]


def read_yaml(path: Path) -> Any:
    return yaml.safe_load(path.read_text(encoding="utf-8"))


@dataclass
class FieldCandidate:
    """A single text field with markup opportunity."""
    frame_path: str
    node_id: str
    node_kind: str
    field_name: str
    current_format: str  # "plain", "md-inline", "md-block", or inferred
    has_code_backticks: bool
    has_paths: bool
    has_emphasis: bool
    has_references: bool
    has_lists: bool
    text_length: int
    recommendation: str  # "md-inline", "md-block", or "review"


def is_str(x: Any) -> bool:
    return isinstance(x, str) and bool(x)


def find_attr(attrs: Any, key: str) -> Optional[str]:
    if not isinstance(attrs, list):
        return None
    for a in attrs:
        if isinstance(a, dict) and a.get("key") == key:
            v = a.get("value")
            if isinstance(v, str):
                return v
    return None


def detect_patterns(text: str) -> Dict[str, bool]:
    """Detect markup-friendly patterns in text."""
    return {
        "code_backticks": "`" in text,
        "paths": bool(re.search(r"(frames|tools|docs|\.github)/[\w/\-\.]+", text)),
        "emphasis": bool(re.search(r"\*\*\w+\*\*|\*\w+\*|__\w+__|_\w+_", text)),
        "references": bool(re.search(r"(law|spec|frame)://\S+", text)),
        "lists": bool(re.search(r"^[\s]*[-*+]\s+\w", text, re.MULTILINE)),
```

#### tools/no_diff
Source: `tools/no_diff/run`

```
#!/usr/bin/env bash
set -euo pipefail

repo_root="$(cd "$(dirname "$0")/../.." && pwd)"
out_dir="$repo_root/out/no_diff"
mkdir -p "$out_dir"

# Reproducibility gate:
# (A) Render *projections* twice into scratch dirs and assert byte-for-byte identity.
# (B) Render the repo's docs/**/README.md twice (in scratch repos) and assert identity.

work1="$out_dir/work1"
work2="$out_dir/work2"
rm -rf "$work1" "$work2"
mkdir -p "$work1" "$work2"

render_all() {
  local w="$1"
  mkdir -p "$w/docir" "$w/md" "$w/tex"

  local frame1="$repo_root/frames/domains/spec/systemics/sigma-k1/v0.1.0/frame.yml"
  local frame2="$repo_root/frames/domains/spec/systemics/sigma-composition-k1/v0.1.0/frame.yml"

  "$repo_root/tools/render_docir/run" --in "$frame1" --out "$w/docir/sigma-k1-v0.1.0.json"
  "$repo_root/tools/render_md_doc/run" --in "$w/docir/sigma-k1-v0.1.0.json" --out "$w/md/sigma-k1-v0.1.0.md"
  "$repo_root/tools/render_tex_doc/run" --in "$w/docir/sigma-k1-v0.1.0.json" --out-dir "$w/tex/sigma-k1-v0.1.0"

  "$repo_root/tools/render_docir/run" --in "$frame2" --out "$w/docir/sigma-composition-k1-v0.1.0.json"
  "$repo_root/tools/render_md_doc/run" --in "$w/docir/sigma-composition-k1-v0.1.0.json" --out "$w/md/sigma-composition-k1-v0.1.0.md"
  "$repo_root/tools/render_tex_doc/run" --in "$w/docir/sigma-composition-k1-v0.1.0.json" --out-dir "$w/tex/sigma-composition-k1-v0.1.0"
}

digest_tree() {
  local w="$1"
  ( cd "$w" && find . -type f -print0 | LC_ALL=C sort -z | xargs -0 shasum -a 256 )
}

# (A) Projection determinism
render_all "$work1"
render_all "$work2"

d1="$out_dir/digests-1.txt"
d2="$out_dir/digests-2.txt"
digest_tree "$work1" > "$d1"
digest_tree "$work2" > "$d2"

ok=true
note="ok"
if ! diff -u "$d1" "$d2" >/dev/null; then
  ok=false
  note="Outputs differ between runs (non-deterministic projections)"
  diff -u "$d1" "$d2" > "$out_dir/diff.txt" || true
fi

# (B) docs/** determinism (render_docs)
# Use two clean copies of the repo (excluding .git) to avoid clobbering the working tree.
repo1="$out_dir/repo1"
repo2="$out_dir/repo2"
rm -rf "$repo1" "$repo2"
mkdir -p "$repo1" "$repo2"

copy_repo() {
  local dst="$1"
  ( cd "$repo_root" && tar -czf - --exclude .git --exclude out --exclude site . ) | ( cd "$dst" && tar -xzf - )
}

copy_repo "$repo1"
copy_repo "$repo2"

(
  cd "$repo1"
  ./tools/render_docs/run >/dev/null
)
(
  cd "$repo2"
  ./tools/render_docs/run >/dev/null
)

# Hash only the generated docs tree and its manifest.
# (If you want to include more generated material later, expand these paths.)
```

#### tools/pub_build_pdf
Source: `tools/pub_build_pdf/run`

```
#!/usr/bin/env bash
set -euo pipefail

# Build a PDF from a LaTeX bundle.
#
# Prefer `tectonic` (single-binary, good for CI); fall back to `latexmk` if present.
#
# Usage:
#   tools/pub_build_pdf/run --src <dir> --out <dir> --name <basename>

SRC=""
OUT=""
NAME="document"

while [[ $# -gt 0 ]]; do
  case "$1" in
    --src) SRC="$2"; shift 2;;
    --out) OUT="$2"; shift 2;;
    --name) NAME="$2"; shift 2;;
    *) echo "Unknown arg: $1" >&2; exit 2;;
  esac
done

if [[ -z "$SRC" || -z "$OUT" ]]; then
  echo "Missing --src or --out" >&2
  exit 2
fi

mkdir -p "$OUT"

TMP="$(mktemp -d)"
trap 'rm -rf "$TMP"' EXIT

cp -R "$SRC"/* "$TMP"/

build_with_tectonic() {
  if ! command -v tectonic >/dev/null 2>&1; then
    return 1
  fi

  (
    cd "$TMP"
    # `--print` avoids extra chatter; `--outdir` keeps outputs together.
    tectonic --print --synctex -Z shell-escape main.tex >/dev/null
  )

  # tectonic writes main.pdf in the working dir
  [[ -f "$TMP/main.pdf" ]]
}

build_with_latexmk() {
  if ! command -v latexmk >/dev/null 2>&1; then
    return 1
  fi

  (
    cd "$TMP"
    latexmk -pdf -interaction=nonstopmode -halt-on-error -file-line-error main.tex >/dev/null
  )

  [[ -f "$TMP/main.pdf" ]]
}

if build_with_tectonic; then
  :
elif build_with_latexmk; then
  :
else
  echo "Neither tectonic nor latexmk found. In CI install tectonic (recommended)." >&2
  exit 1
fi

cp "$TMP/main.pdf" "$OUT/${NAME}.pdf"

(
  cd "$OUT"
  shasum -a 256 "${NAME}.pdf" > SHA256SUMS
)
```

#### tools/pub_manifest
Source: `tools/pub_manifest/run.py`

```
#!/usr/bin/env python3
"""Write a deterministic publication manifest.

Usage:
  tools/pub_manifest/run.py --frame <frame.yml> --src <srcdir> --pdf <pdfpath> --out <MANIFEST.json>

The manifest records hashes and build context (commit SHA if available).
"""

from __future__ import annotations

import argparse
import hashlib
import json
import os
import subprocess
from pathlib import Path
from typing import Any, Dict


def sha256_file(path: Path) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()


def git_head_sha(root: Path) -> str:
    try:
        out = subprocess.check_output(["git", "rev-parse", "HEAD"], cwd=str(root))
        return out.decode("utf-8").strip()
    except Exception:
        return ""


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--frame", required=True)
    ap.add_argument("--src", required=True)
    ap.add_argument("--pdf", required=True)
    ap.add_argument("--out", required=True)
    args = ap.parse_args()

    repo_root = Path.cwd()
    frame = Path(args.frame)
    src = Path(args.src)
    pdf = Path(args.pdf)
    out = Path(args.out)

    data: Dict[str, Any] = {
        "version": "0.1.0",
        "commit": git_head_sha(repo_root),
        "inputs": {
            "frame_path": str(frame),
            "frame_sha256": sha256_file(frame),
        },
        "outputs": {
            "main_tex_path": str(src / "main.tex"),
            "main_tex_sha256": sha256_file(src / "main.tex"),
            "pdf_path": str(pdf),
            "pdf_sha256": sha256_file(pdf),
        },
        "tooling": {
            "render_latex_spec": "0.1.0",
            "pub_build_pdf": "0.1.0",
        },
    }

    out.parent.mkdir(parents=True, exist_ok=True)
    out.write_text(json.dumps(data, indent=2, sort_keys=True) + "\n", encoding="utf-8")


if __name__ == "__main__":
    main()
```

#### tools/render_docir
Source: `tools/render_docir/run.py`

```
#!/usr/bin/env python3
"""Render a GF0/SpecFrame YAML into a deterministic DocIR JSON.

DocIR is a linear intermediate representation intended for human-doc projections.
Target renderers (Markdown/LaTeX/plaintext) should be pure printers over DocIR.

Usage:
  tools/render_docir/run.py --in <frame.yml> --out <docir.json>

Determinism:
- stable ordering (order -> id)
- no timestamps
- stable anchor derivation

This first implementation supports the SpecFrame-ish subset used in this repo.
"""

from __future__ import annotations

import argparse
import hashlib
import json
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import yaml

# Ensure repo root is on sys.path so we can import tools/* as modules.
REPO_ROOT = Path(__file__).resolve().parents[2]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

# InlineMarkup-K1 (deterministic tiny markup subset)
try:
    from tools.markup.inline_markup_k1 import parse as parse_inline_markup  # type: ignore
except Exception:  # pragma: no cover
    parse_inline_markup = None  # type: ignore


def sha256_bytes(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()


def read_bytes(p: Path) -> bytes:
    return p.read_bytes()


_LATEXISH_WS = re.compile(r"\s+")


def norm_text(s: str) -> str:
    # Preserve newlines as paragraph boundaries; normalize CRLF.
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    # Trim trailing whitespace per line.
    s = "\n".join(ln.rstrip() for ln in s.split("\n"))
    return s.strip("\n")


def slugify(s: str) -> str:
    s = s.strip().lower()
    s = re.sub(r"[^a-z0-9]+", "-", s)
    s = re.sub(r"-+", "-", s).strip("-")
    return s or "x"


def stable_anchor(node_id: str) -> str:
    # Anchor should be readable but collision-resistant.
    base = slugify(node_id)
    h = hashlib.sha256(node_id.encode("utf-8")).hexdigest()[:8]
    return f"{base}-{h}"


def find_attr(attrs: Any, key: str) -> Optional[str]:
    if not isinstance(attrs, list):
        return None
    for a in attrs:
        if isinstance(a, dict) and a.get("key") == key:
```

#### tools/render_docs
Source: `tools/render_docs/run`

```
#!/usr/bin/env bash
set -euo pipefail

repo_root="$(cd "$(dirname "$0")/../.." && pwd)"
mkdir -p "$repo_root/docs" "$repo_root/out/render_docs"

# Deterministically render docs for all canonical frames in frames/**/v*/frame.yml.
# New pipeline: GF0/SpecFrame -> DocIR -> Markdown.
python3 - <<'PY'
import json
from pathlib import Path

root = Path('.').resolve()
frames = sorted(root.glob('frames/**/v*/frame.yml'))

def frameurl_path(graph_id: str) -> str:
    # frameurl-ish: <scheme>://<scope>/<segments...> -> <scope>/<scheme>/<segments...>
    if '://' not in graph_id:
        return graph_id
    scheme, rest = graph_id.split('://', 1)
    scope = rest.split('/', 1)[0]
    tail = rest.split('/', 1)[1] if '/' in rest else ''
    return f"{scope}/{scheme}/{tail}".rstrip('/')

outputs = []
fails = []

docs_root = root / 'docs'
out_docir = root / 'out' / 'render_docs' / 'docir'
out_md = root / 'out' / 'render_docs' / 'md'
out_docir.mkdir(parents=True, exist_ok=True)
out_md.mkdir(parents=True, exist_ok=True)

for p in frames:
    rel = p.relative_to(root).as_posix()
    try:
        import yaml  # type: ignore
        data = yaml.safe_load(p.read_text(encoding='utf-8'))
        if not isinstance(data, dict):
            continue

        gid = data.get('graph_id')
        ver = data.get('version')

        # fallback: first node id and root-node version
        if not isinstance(gid, str) or not gid:
            nodes0 = data.get('nodes') if isinstance(data.get('nodes'), list) else []
            for n in nodes0:
                if isinstance(n, dict) and isinstance(n.get('id'), str) and n.get('id'):
                    gid = n.get('id')
                    break
        if not isinstance(ver, str) or not ver:
            nodes0 = data.get('nodes') if isinstance(data.get('nodes'), list) else []
            if isinstance(gid, str) and gid:
                for n in nodes0:
                    if isinstance(n, dict) and n.get('id') == gid and isinstance(n.get('version'), str) and n.get('version'):
                        ver = n.get('version')
                        break

        if not isinstance(gid, str) or not gid or not isinstance(ver, str) or not ver:
            raise ValueError('missing graph_id/version')

        safe = frameurl_path(gid).replace('/', '__')
        docir_path = out_docir / f"{safe}__v{ver}.json"
        md_path = out_md / f"{safe}__v{ver}.md"

        # Run tools via the repo's wrappers for consistent env/shebang behavior.
        import subprocess
        subprocess.run([str(root / 'tools' / 'render_docir' / 'run'), '--in', str(p), '--out', str(docir_path)], check=True)
        subprocess.run([str(root / 'tools' / 'render_md_doc' / 'run'), '--in', str(docir_path), '--out', str(md_path)], check=True)

        out_path = docs_root / frameurl_path(gid) / f"v{ver}" / 'README.md'
        out_path.parent.mkdir(parents=True, exist_ok=True)
        out_path.write_text(md_path.read_text(encoding='utf-8'), encoding='utf-8', newline='\n')

        outputs.append({
            'graph_id': gid,
            'version': ver,
            'input': rel,
            'docir': docir_path.relative_to(root).as_posix(),
```

#### tools/render_latex_spec
Source: `tools/render_latex_spec/run.py`

```
#!/usr/bin/env python3
"""Deterministically render a SpecFrame K1 'spec' node into a single LaTeX file.

This is intentionally minimal: it emits a self-contained 'main.tex' using stable ordering
(section order, then contains edges) and stable escaping.

Usage:
  tools/render_latex_spec/run.py --in <frame.yml> --out <dir>

Outputs:
  <out>/main.tex

Notes:
  - No timestamps are written inside the LaTeX source (determinism).
  - This renderer supports the node kinds used by SpecFrame K1 in this repo:
    section, clause, term, property, spec_ref.
"""

from __future__ import annotations

import argparse
import os
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import yaml


_LATEX_SPECIALS = {
    "\\": r"\textbackslash{}",
    "{": r"\{",
    "}": r"\}",
    "#": r"\#",
    "%": r"\%",
    "&": r"\&",
    "_": r"\_",
    "^": r"\textasciicircum{}",
    "~": r"\textasciitilde{}",
}


def latex_escape(text: str) -> str:
    # Preserve unicode math-ish symbols (Σ, χ, ⪯, →, ×). Escape only LaTeX specials.
    return "".join(_LATEX_SPECIALS.get(ch, ch) for ch in text)


def norm_ws(s: str) -> str:
    # Keep text blocks readable; avoid trailing whitespace churn.
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    return "\n".join(line.rstrip() for line in s.split("\n")).strip("\n")


def load_frame(path: Path) -> Dict[str, Any]:
    with path.open("r", encoding="utf-8") as f:
        return yaml.safe_load(f)


@dataclass(frozen=True)
class Node:
    id: str
    kind: str
    status: str
    title: Optional[str] = None
    label: Optional[str] = None
    order: Optional[int] = None
    text: Optional[str] = None
    summary: Optional[str] = None
    symbols: Optional[List[Dict[str, str]]] = None
    target_graph_id: Optional[str] = None


def parse_nodes(g: Dict[str, Any]) -> Dict[str, Node]:
    out: Dict[str, Node] = {}
    for raw in g.get("nodes", []):
        out[raw["id"]] = Node(
            id=raw["id"],
            kind=raw.get("kind", ""),
            status=raw.get("status", ""),
```

#### tools/render_md_doc
Source: `tools/render_md_doc/run.py`

```
#!/usr/bin/env python3
"""Render DocIR JSON to deterministic Markdown.

Usage:
  tools/render_md_doc/run.py --in <docir.json> --out <doc.md>

Notes:
- This is a pure pretty-printer over DocIR.
- Deterministic ordering comes from DocIR (already ordered blocks).
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List


def read_json(path: Path) -> Dict[str, Any]:
    return json.loads(path.read_text(encoding="utf-8"))


def md_escape(s: str) -> str:
    # Conservative escaping for plain-text fields.
    # Avoid changing unicode.
    s = s.replace("\\", "\\\\")
    s = s.replace("`", "\\`")
    s = s.replace("*", "\\*")
    s = s.replace("_", "\\_")
    s = s.replace("[", "\\[").replace("]", "\\]")
    return s


def render_inline_markup_k1(ast: Dict[str, Any]) -> str:
    # InlineMarkup-K1 blocks -> Markdown
    # We render paragraphs separated by blank lines and code fences verbatim.
    out: List[str] = []
    blocks = ast.get("blocks") or []
    for b in blocks:
        if not isinstance(b, dict):
            continue
        t = b.get("t")
        if t == "code_fence":
            lang = str(b.get("lang") or "").strip()
            code = str(b.get("code") or "")
            out.append(f"```{lang}".rstrip())
            out.extend(code.split("\n"))
            out.append("```")
            out.append("")
        elif t == "paragraph":
            out.append(render_inline_nodes_md(b.get("c") or []))
            out.append("")
        else:
            out.append(f"[unhandled:{t}]")
            out.append("")
    return "\n".join(out).rstrip("\n")


def render_inline_nodes_md(nodes: List[Dict[str, Any]]) -> str:
    out: List[str] = []
    for n in nodes:
        if not isinstance(n, dict):
            continue
        t = n.get("t")
        if t == "text":
            out.append(str(n.get("s", "")))
        elif t == "emph":
            out.append("*" + render_inline_nodes_md(n.get("c") or []) + "*")
        elif t == "strong":
            out.append("**" + render_inline_nodes_md(n.get("c") or []) + "**")
        elif t == "code":
            out.append("`" + str(n.get("s", "")) + "`")
        elif t == "math":
            out.append("$" + str(n.get("s", "")) + "$")
        elif t == "link":
            out.append("[" + render_inline_nodes_md(n.get("c") or []) + "](" + str(n.get("url", "")) + ")")
        else:
            out.append(str(n.get("s", "")))
    return "".join(out)
```

#### tools/render_simple_md
Source: `tools/render_simple_md/run.py`

```
#!/usr/bin/env python3
"""Deterministically render a GF0 frame to Markdown (Simple Markdown Renderer K1).

This is a repo-local deterministic renderer.

Input:
- frame file path (YAML) OR reads all frames in frames/**/v*/frame.yml when no args are given.

Output (default):
- docs/<frameurl_path>/v<version>/README.md

Also writes:
- out/render_simple_md/report.json

Notes:
- LF newlines
- strip trailing whitespace
- exactly one trailing LF
"""

from __future__ import annotations

import json
import sys
from pathlib import Path

try:
    import yaml  # type: ignore
except Exception:
    print("ERROR: PyYAML is required (pip install pyyaml)", file=sys.stderr)
    raise


_FRAME_LEAF = "frame.yml"


def _is_str(x) -> bool:
    return isinstance(x, str) and bool(x)


def _as_list(x):
    return x if isinstance(x, list) else []


def _stable_json(x) -> str:
    return json.dumps(x, sort_keys=True, separators=(",", ":"), ensure_ascii=False)


def _frameurl_path(graph_id: str) -> str:
    # render://_kernel/md/simple-k1 -> _kernel/render/md/simple-k1
    # <scheme>://<scope>/<segments...>
    if "://" not in graph_id:
        return graph_id
    scheme, rest = graph_id.split("://", 1)
    return f"{rest.split('/',1)[0]}/{scheme}/{rest.split('/',1)[1] if '/' in rest else ''}".rstrip("/")


def _yaml_block(obj: dict) -> str:
    # Deterministic YAML code block: stable key order.
    # PyYAML doesn't guarantee ordering; we pre-sort dict keys recursively.
    def sort_obj(o):
        if isinstance(o, dict):
            return {k: sort_obj(o[k]) for k in sorted(o.keys())}
        if isinstance(o, list):
            return [sort_obj(i) for i in o]
        return o

    s = yaml.safe_dump(
        sort_obj(obj),
        sort_keys=False,  # already sorted
        allow_unicode=True,
        default_flow_style=False,
        width=88,
    )
    return s.rstrip("\n")


def _validate_gf0(frame: dict) -> list[dict]:
    errs: list[dict] = []
```

#### tools/render_tex_doc
Source: `tools/render_tex_doc/run.py`

```
#!/usr/bin/env python3
"""Render DocIR JSON to a deterministic LaTeX bundle.

Outputs a directory containing:
- main.tex

Usage:
  tools/render_tex_doc/run.py --in <docir.json> --out-dir <latex_dir>

Notes:
- Pure pretty-printer over DocIR.
- Designed to be consumed by tools/pub_build_pdf/run.
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List


def read_json(path: Path) -> Dict[str, Any]:
    return json.loads(path.read_text(encoding="utf-8"))


def tex_escape(s: str) -> str:
    # Minimal escaping for plain text fields.
    return (
        s.replace("\\", r"\textbackslash{}")
        .replace("{", r"\{")
        .replace("}", r"\}")
        .replace("$", r"\$")
        .replace("&", r"\&")
        .replace("#", r"\#")
        .replace("%", r"\%")
        .replace("_", r"\_")
        .replace("~", r"\textasciitilde{}")
        .replace("^", r"\textasciicircum{}")
    )


def render_inline_nodes_tex(nodes: List[Dict[str, Any]]) -> str:
    out: List[str] = []
    for n in nodes:
        if not isinstance(n, dict):
            continue
        t = n.get("t")
        if t == "text":
            out.append(tex_escape(str(n.get("s", ""))))
        elif t == "emph":
            out.append(r"\emph{" + render_inline_nodes_tex(n.get("c") or []) + "}")
        elif t == "strong":
            out.append(r"\textbf{" + render_inline_nodes_tex(n.get("c") or []) + "}")
        elif t == "code":
            out.append(r"\texttt{" + tex_escape(str(n.get("s", ""))) + "}")
        elif t == "math":
            out.append("$" + str(n.get("s", "")) + "$")
        elif t == "link":
            out.append(r"\href{" + tex_escape(str(n.get("url", ""))) + "}{" + render_inline_nodes_tex(n.get("c") or []) + "}")
        else:
            out.append(tex_escape(str(n.get("s", ""))))
    return "".join(out)


def render_inline_markup_k1_tex(ast: Dict[str, Any]) -> List[str]:
    out: List[str] = []
    blocks = ast.get("blocks") or []
    for b in blocks:
        if not isinstance(b, dict):
            continue
        t = b.get("t")
        if t == "code_fence":
            code = str(b.get("code") or "")
            out.append(r"\begin{verbatim}")
            out.extend(code.split("\n"))
            out.append(r"\end{verbatim}")
            out.append("")
        elif t == "paragraph":
            out.append(render_inline_nodes_tex(b.get("c") or []))
```

#### tools/run_with_timeout
Source: `tools/run_with_timeout/run.py`

```
#!/usr/bin/env python3
"""Run a command with a hard timeout and minimal, deterministic reporting.

This is intended to mitigate wedged or long-running gates in CI and locally.

Usage:
  tools/run_with_timeout/run.py --seconds 120 --out out/<gate>/log.txt -- <cmd> [args...]

Exit codes:
  - returns the child process exit code if it completes
  - returns 124 on timeout

Notes:
  - Writes combined stdout/stderr to the log file.
  - Also prints a short tail to stdout for quick visibility.
"""

from __future__ import annotations

import argparse
import os
import subprocess
import sys
from pathlib import Path


def tail_lines(s: str, n: int) -> str:
    lines = s.splitlines()
    if len(lines) <= n:
        return s
    return "\n".join(lines[-n:])


def main() -> None:
    ap = argparse.ArgumentParser(add_help=True)
    ap.add_argument("--seconds", type=int, required=True)
    ap.add_argument("--out", required=True)
    ap.add_argument("--tail", type=int, default=80)
    ap.add_argument("cmd", nargs=argparse.REMAINDER)
    args = ap.parse_args()

    if not args.cmd or args.cmd[0] != "--" or len(args.cmd) < 2:
        print("Expected: -- <cmd> [args...]", file=sys.stderr)
        sys.exit(2)

    cmd = args.cmd[1:]
    out_path = Path(args.out)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    # Ensure non-interactive defaults.
    env = dict(os.environ)
    env.setdefault("GIT_PAGER", "cat")
    env.setdefault("PAGER", "cat")
    env.setdefault("GIT_TERMINAL_PROMPT", "0")

    try:
        p = subprocess.run(
            cmd,
            cwd=os.getcwd(),
            env=env,
            capture_output=True,
            text=True,
            timeout=args.seconds,
            check=False,
        )
        combined = (p.stdout or "") + (p.stderr or "")
        out_path.write_text(combined, encoding="utf-8")
        if combined.strip():
            print(tail_lines(combined, args.tail))
        sys.exit(p.returncode)
    except subprocess.TimeoutExpired as e:
        combined = ""
        if e.stdout:
            combined += e.stdout
        if e.stderr:
            combined += e.stderr
        combined += f"\n[run_with_timeout] TIMEOUT after {args.seconds}s: {' '.join(cmd)}\n"
        out_path.write_text(combined, encoding="utf-8")
        if combined.strip():
            print(tail_lines(combined, args.tail))
```

#### tools/semantic_invariants
Source: `tools/semantic_invariants/run.py`

```
#!/usr/bin/env python3
"""Verify semantic invariants are preserved across frame edits.

This tool compares two frame.yml files (before/after) and asserts that
only "safe" fields were changed (e.g., text.format, text, summary).
Structural changes (IDs, edges, kinds, statuses) are forbidden.

Usage:
  tools/semantic_invariants/run.py --before <old.yml> --after <new.yml> [--verbose]

Exit codes:
  0 if invariants are preserved
  1 if structural changes detected

Allowed changes:
- text, summary, title (content only, not presence/absence)
- text.format attribute
- description fields (desc)
- Other doc-oriented attrs

Forbidden changes:
- graph_id, version
- node id, kind, status, profile, law_id, law_version
- edges (from, to, type)
- machine-consumed fields (symbols, validators, etc.)
- Removing/adding nodes

Design:
- Deterministic
- Clear whitelist of safe field changes
- Reports violations clearly
"""

from __future__ import annotations

import argparse
import json
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import yaml

REPO_ROOT = Path(__file__).resolve().parents[2]


def read_yaml(path: Path) -> Any:
    return yaml.safe_load(path.read_text(encoding="utf-8"))


# Fields that are safe to modify for readability
SAFE_TEXT_FIELDS = {"text", "summary", "title", "label", "desc"}

# Fields that MUST NOT change
PROTECTED_FIELDS = {
    "graph_id",
    "version",
    "id",
    "kind",
    "status",
    "profile",
    "law_id",
    "law_version",
}


@dataclass
class Violation:
    """A structural invariant violation."""
    code: str  # e.g., "node_removed", "edge_added", "id_changed"
    frame_path: str
    details: str


def normalize_node_id(node: Dict[str, Any]) -> str:
    """Extract a stable node ID."""
    nid = node.get("id")
    return str(nid) if nid else ""
```

#### tools/validate_group
Source: `tools/validate_group/run`

```
#!/usr/bin/env bash
set -euo pipefail

repo_root="$(cd "$(dirname "$0")/../.." && pwd)"
out_dir="$repo_root/out/validate_group"
mkdir -p "$out_dir"

report="$out_dir/report.json"

# Bootstrap-only: deterministic DocGroup scan per RepoLaw K1.
# Full GF0/profile validation is out of scope for this stub.

frames=()
while IFS= read -r p; do
  frames+=("$p")
done < <(
  find "$repo_root/frames" -type f -path "*/v*/frame.yml" -print | LC_ALL=C sort
)

# Emit deterministic JSON report
{
  printf '{\n'
  printf '  "tool": {"id": "validate_group", "version": "0.1.0"},\n'
  printf '  "docgroup": {"frame_count": %s},\n' "${#frames[@]}"
  printf '  "frames": [\n'
  for i in "${!frames[@]}"; do
    rel="${frames[$i]#"$repo_root/"}"
    [ $i -gt 0 ] && printf ',\n'
    printf '    {"path": "%s"}' "$rel"
  done
  printf '\n  ]\n'
  printf '}\n'
} > "$report"

# Always succeed for the bootstrap stub.
exit 0
```

#### tools/validate_inline_markup
Source: `tools/validate_inline_markup/run.py`

```
#!/usr/bin/env python3
"""Validate InlineMarkup-K1 in frames.

This is a lightweight gate that enforces:
- Disallow raw HTML-ish characters (< or >)
- Disallow block constructs in md-inline
- Ensure delimiters are balanced for supported constructs

Policy (v0.1):
- If node attrs contain text.format, validate the relevant text-like fields.
- Fields validated: text, summary, desc

Exit codes:
- 0 if ok
- 1 if any violations

Writes:
- out/validate_inline_markup/report.json
"""

from __future__ import annotations

import json
import sys
from pathlib import Path
from typing import Any, Dict, List, Tuple

import yaml

# Ensure repo root is on sys.path so we can import tools/* as modules.
REPO_ROOT = Path(__file__).resolve().parents[2]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from tools.markup.inline_markup_k1 import parse  # type: ignore


def is_str(x: Any) -> bool:
    return isinstance(x, str) and bool(x)


def find_attr(attrs: Any, key: str) -> str | None:
    if not isinstance(attrs, list):
        return None
    for a in attrs:
        if isinstance(a, dict) and a.get("key") == key:
            v = a.get("value")
            if isinstance(v, str):
                return v
    return None


def iter_frames(root: Path) -> List[Path]:
    return sorted(root.glob("frames/**/v*/frame.yml"))


def validate_frame(path: Path) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """Return (violations, warnings)."""

    data = yaml.safe_load(path.read_text(encoding="utf-8"))
    if not isinstance(data, dict):
        return ([{"code": "TEXT.E.BAD_FRAME", "message": "Frame is not a mapping"}], [])

    nodes = data.get("nodes")
    if not isinstance(nodes, list):
        return ([], [])

    violations: List[Dict[str, Any]] = []
    warnings: List[Dict[str, Any]] = []

    for n in nodes:
        if not isinstance(n, dict):
            continue
        nid = str(n.get("id") or "")
        fmt = find_attr(n.get("attrs"), "text.format") or "plain"

        for field in ("text", "summary", "desc"):
            v = n.get(field)
            if not is_str(v):
                continue
```

#### tools/validate_references
Source: `tools/validate_references/run.py`

```
#!/usr/bin/env python3
"""Deterministically validate that *FrameURL* references resolve within the repo.

This repo contains many intra-frame edges that reference local node ids like
`section.1.charter`. Those are NOT FrameURLs and must not be validated as
cross-frame references.

Inputs:
- frames/**/v*/frame.yml

Checks:
- graph_id present
- root node exists with id == graph_id
- FrameURL-ish references resolve via the present canonical graph_id set

Validates FrameURL references found in:
- properties[].{key,value} where key == "depends_on"
- target_graph_id
- edges[].from (must equal graph_id; if it's FrameURL-ish and not equal, must still resolve)

Emits: out/validate_references/report.json
Exit code 1 on any unresolved references.
"""

from __future__ import annotations

import json
import re
import sys
from pathlib import Path

try:
    import yaml  # type: ignore
except Exception:
    print("ERROR: PyYAML is required (pip install pyyaml)", file=sys.stderr)
    raise


_FRAMEURL_RE = re.compile(r"^[a-z][a-z0-9+.-]*://")


def is_frameurl(value: str) -> bool:
    return bool(_FRAMEURL_RE.match(value))


def _iter_frames(root: Path):
    for p in sorted(root.glob("frames/**/v*/frame.yml")):
        yield p


def main() -> int:
    root = Path(".").resolve()

    # Build set of canonical graph_ids present.
    present_graph_ids: set[str] = set()

    frames_data: list[tuple[Path, dict]] = []
    for p in _iter_frames(root):
        data = yaml.safe_load(p.read_text(encoding="utf-8"))
        if not isinstance(data, dict):
            continue
        gid = data.get("graph_id")
        if isinstance(gid, str) and gid:
            present_graph_ids.add(gid)
        frames_data.append((p, data))

    def resolves(ref: str) -> bool:
        return ref in present_graph_ids

    failures: list[dict] = []

    for p, data in frames_data:
        rel = p.relative_to(root).as_posix()
        gid = data.get("graph_id")
        if not isinstance(gid, str) or not gid:
            failures.append({"path": rel, "type": "missing_graph_id"})
            continue

        # root node check (accept either id == graph_id, or id is a dict with value == graph_id)
        root_node_ok = False
```
